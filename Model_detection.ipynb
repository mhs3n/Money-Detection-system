{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7094d514-d3b7-4910-a51a-b5de3d456f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-304-g22361691 Python-3.10.12 torch-2.0.1+cu117 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete ✅ (4 CPUs, 3.7 GB RAM, 53.2/249.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt comet_ml  # install\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d379c32-cf7e-4028-8569-fc1abfad6d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhsn/Files/PFA\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82211ddb-ddd5-443c-b03b-d23b78f0df86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhsn/Files/PFA/yolov5/yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd /home/mhsn/Files/PFA/yolov5/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb45689-16f1-4ec9-8b0d-c44544b70da8",
   "metadata": {},
   "source": [
    "# Fixing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f9665-0448-4c0c-bac9-a1ba0ddc76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, random\n",
    "\n",
    "# preparing the folder structure\n",
    "\n",
    "full_data_path = 'data/obj/'\n",
    "extension_allowed = '.jpg'\n",
    "split_percentage = 90\n",
    "\n",
    "images_path = 'data/images/'\n",
    "if os.path.exists(images_path):\n",
    "    shutil.rmtree(images_path)\n",
    "os.mkdir(images_path)\n",
    "    \n",
    "labels_path = 'data/labels/'\n",
    "if os.path.exists(labels_path):\n",
    "    shutil.rmtree(labels_path)\n",
    "os.mkdir(labels_path)\n",
    "    \n",
    "training_images_path = images_path + 'training/'\n",
    "validation_images_path = images_path + 'validation/'\n",
    "training_labels_path = labels_path + 'training/'\n",
    "validation_labels_path = labels_path +'validation/'\n",
    "    \n",
    "os.mkdir(training_images_path)\n",
    "os.mkdir(validation_images_path)\n",
    "os.mkdir(training_labels_path)\n",
    "os.mkdir(validation_labels_path)\n",
    "\n",
    "files = []\n",
    "\n",
    "ext_len = len(extension_allowed)\n",
    "\n",
    "for r, d, f in os.walk(full_data_path):\n",
    "    for file in f:\n",
    "        if file.endswith(extension_allowed):\n",
    "            strip = file[0:len(file) - ext_len]      \n",
    "            files.append(strip)\n",
    "\n",
    "random.shuffle(files)\n",
    "\n",
    "size = len(files)                   \n",
    "\n",
    "split = int(split_percentage * size / 100)\n",
    "\n",
    "print(\"copying training data\")\n",
    "for i in range(split):\n",
    "    strip = files[i]\n",
    "                         \n",
    "    image_file = strip + extension_allowed\n",
    "    src_image = full_data_path + image_file\n",
    "    shutil.copy(src_image, training_images_path) \n",
    "                         \n",
    "    annotation_file = strip + '.txt'\n",
    "    src_label = full_data_path + annotation_file\n",
    "    shutil.copy(src_label, training_labels_path) \n",
    "\n",
    "print(\"copying validation data\")\n",
    "for i in range(split, size):\n",
    "    strip = files[i]\n",
    "                         \n",
    "    image_file = strip + extension_allowed\n",
    "    src_image = full_data_path + image_file\n",
    "    shutil.copy(src_image, validation_images_path) \n",
    "                         \n",
    "    annotation_file = strip + '.txt'\n",
    "    src_label = full_data_path + annotation_file\n",
    "    shutil.copy(src_label, validation_labels_path) \n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095391a7-9982-4714-b965-cca454a9f902",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51bc5440-e2db-4a0b-a503-4734f2b9a49f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1353071617.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python3 train.py --img 243 --epochs 30 --data dataset.yaml --weights yolov5s.pt --cache\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python3 train.py --img 243 --epochs 30 --data dataset.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2051a1-666b-4651-b0d9-cb95e0ba21c5",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e6927a-82b5-482f-a0d2-a2e537b8029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9f3fa6-6b1f-4850-babe-c6bc0926e29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /home/mhsn/.cache/torch/hub/master.zip\n",
      "YOLOv5 🚀 2024-5-12 Python-3.10.12 torch-2.0.1+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='/home/mhsn/Files/PFA/yolov5/best.pt', force_reload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf68d59-96ca-4631-83fd-28e597053f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd23433-857b-4003-bbe6-7122f04e4a59",
   "metadata": {},
   "source": [
    "# Testing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a39f2-642f-4b67-aa6c-fed09c3de773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "img=cv2.imread('/home/mhsn/Files/PFA/Dataset/Train/10dt/yoPJx7ljbZUAFoOpvrTWkfgQk7f.jpg')\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60343bad-a185-4eb4-af7a-d0fba6a4bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = cv2.imread('/home/mhsn/Files/PFA/Dataset/Train/10dt/yoPJx7ljbZUAFoOpvrTWkfgQk7f.jpg')\n",
    "#getting the results\n",
    "result=model(image)\n",
    "predictions = result.pred[0]\n",
    "filtered_predictions = [pred for pred in predictions if pred[4] >= 0.7]\n",
    "for pred in filtered_predictions:\n",
    "    # Extract bounding box coordinates from the prediction\n",
    "    x1, y1, x2, y2 = map(int, pred[:4].tolist())\n",
    "\n",
    "    # Extract class index from the prediction\n",
    "    class_index = int(pred[-1].item())\n",
    "\n",
    "    # Retrieve the corresponding value from the mapping dictionary\n",
    "    value = class_value_mapping.get(class_index)\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Display class label and value\n",
    "    label = f\"Class: {class_index}, Value: {value} dt\"\n",
    "    cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "print(x1,x2,y1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccd79b-7ac7-42f3-9033-cfc25a063bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('/home/mhsn/Files/PFA/Dataset/Train/10dt/yoPJx7ljbZUAFoOpvrTWkfgQk7f.jpg')\n",
    "# Get results\n",
    "result=model(image)\n",
    "\n",
    "# Assuming result.pred[0] contains the tensor you provided\n",
    "predictions = result.pred[0]\n",
    "\n",
    "# Define the mapping dictionary\n",
    "class_value_mapping = {\n",
    "    0: 10,  # Class 0 corresponds to 10dt\n",
    "    1: 20,  # Class 1 corresponds to 20dt\n",
    "    2: 50,  # Class 2 corresponds to 50dt\n",
    "    3: 5    # Class 3 corresponds to 5dt\n",
    "}\n",
    "\n",
    "# Filter predictions with confidence below 0.7\n",
    "filtered_predictions = [pred for pred in predictions if pred[4] >= 0.7]\n",
    "\n",
    "# Iterate through each filtered prediction\n",
    "for pred in filtered_predictions:\n",
    "    # Extract bounding box coordinates from the prediction\n",
    "    x1, y1, x2, y2 = map(int, pred[:4].tolist())\n",
    "\n",
    "    # Extract class index from the prediction\n",
    "    class_index = int(pred[-1].item())\n",
    "\n",
    "    # Retrieve the corresponding value from the mapping dictionary\n",
    "    value = class_value_mapping.get(class_index)\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Display class label and value\n",
    "    label = f\"Class: {class_index}, Value: {value} dt\"\n",
    "    cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Convert image from BGR to RGB (required for displaying with plt.show)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image using plt.show\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ee791-ca45-4024-b4eb-3c03d1a5ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = cv2.imread('/home/mhsn/Files/PFA/Dataset/Train/10dt/yoPJx7ljbZUAFoOpvrTWkfgQk7f.jpg')\n",
    "#getting the results\n",
    "result=model(image)\n",
    "\n",
    "# Assuming result.pred[0] contains the tensor you provided\n",
    "predictions = result.pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe0bc8-a9f0-4cb6-bbe2-e4b4f3112695",
   "metadata": {},
   "source": [
    "# testing on an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e616186-2277-4625-b92d-72d68ac4157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Define the mapping dictionary\n",
    "class_value_mapping = {\n",
    "    0: 10,  # Class 0 corresponds to 10dt\n",
    "    1: 20,  # Class 1 corresponds to 20dt\n",
    "    2: 50,  # Class 2 corresponds to 50dt\n",
    "    3: 5    # Class 3 corresponds to 5dt\n",
    "}\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('/home/mhsn/Files/PFA/Dataset/Train/10dt/yoPJx7ljbZUAFoOpvrTWkfgQk7f.jpg')\n",
    "#getting the results\n",
    "result=model(image)\n",
    "\n",
    "# Assuming result.pred[0] contains the tensor you provided\n",
    "predictions = result.pred[0]\n",
    "\n",
    "# Iterate through each prediction\n",
    "for pred in predictions:\n",
    "    # Extract confidence score from the prediction\n",
    "    confidence = pred[4].item()\n",
    "\n",
    "    # Only process predictions with confidence over 0.6\n",
    "    if confidence > 0.6:\n",
    "        # Extract bounding box coordinates from the prediction\n",
    "        x1, y1, x2, y2 = map(int, pred[:4].tolist())\n",
    "\n",
    "        # Extract class index from the prediction\n",
    "        class_index = int(pred[-1].item())\n",
    "\n",
    "        # Retrieve the corresponding value from the mapping dictionary\n",
    "        value = class_value_mapping.get(class_index)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Display class label and value\n",
    "        label = f\"Class: {class_index}, Value: {value} dt\"\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Show the image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c86d92-2774-4f75-8420-ad223aae3f22",
   "metadata": {},
   "source": [
    "# working on real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcfc899c-f7d5-4309-88bb-8788ef3e8f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n",
      "WARNING ⚠️ NMS time limit 0.550s exceeded\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Define the mapping dictionary\n",
    "class_value_mapping = {\n",
    "    0: 10,  # Class 0 corresponds to 10dt\n",
    "    1: 20,  # Class 1 corresponds to 20dt\n",
    "    2: 50,  # Class 2 corresponds to 50dt\n",
    "    3: 5    # Class 3 corresponds to 5dt\n",
    "}\n",
    "\n",
    "# Load the pre-trained model\n",
    "# Assuming model is already loaded and ready for inference\n",
    "\n",
    "# Open a video capture device (webcam or video file)\n",
    "cap = cv2.VideoCapture(0)  # Change to 0 for webcam, or provide video file path\n",
    "\n",
    "# Check if the capture device is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Failed to open video capture device.\")\n",
    "    exit()\n",
    "\n",
    "# Loop for real-time processing\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame is empty (end of video)\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform inference on the frame\n",
    "    result = model(frame)\n",
    "\n",
    "    # Assuming result.pred[0] contains the tensor you provided\n",
    "    predictions = result.pred[0]\n",
    "\n",
    "    # Iterate through each prediction\n",
    "    for pred in predictions:\n",
    "        # Extract confidence score from the prediction\n",
    "        confidence = pred[4].item()\n",
    "\n",
    "        # Only process predictions with confidence over 0.6\n",
    "        if confidence > 0.6:\n",
    "            # Extract bounding box coordinates from the prediction\n",
    "            x1, y1, x2, y2 = map(int, pred[:4].tolist())\n",
    "\n",
    "            # Extract class index from the prediction\n",
    "            class_index = int(pred[-1].item())\n",
    "\n",
    "            # Retrieve the corresponding value from the mapping dictionary\n",
    "            value = class_value_mapping.get(class_index)\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Display class label and value\n",
    "            label = f\"Class: {class_index}, Value: {value} dt\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Check for 'q' key press to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture device and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d1289d-d1f4-4e2f-aca1-27c1216ed040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory has write permission.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import stat\n",
    "\n",
    "# Define the output directory\n",
    "output_directory = \"/home/mhsn/Files/PFA/Output\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(output_directory):\n",
    "    print(\"Error: The specified directory does not exist.\")\n",
    "    exit()\n",
    "\n",
    "# Check if the directory has write permission\n",
    "if not os.access(output_directory, os.W_OK):\n",
    "    print(\"Error: The directory does not have write permission.\")\n",
    "\n",
    "    # Try to add write permission\n",
    "    try:\n",
    "        os.chmod(output_directory, stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH)\n",
    "        print(\"Write permission added to the directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to add write permission to the directory: {e}\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"The directory has write permission.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b77d71-85c1-4d6d-8f06-f7836cfdee51",
   "metadata": {},
   "source": [
    "# RUN THIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfaa435e-4f21-4b52-b108-300126d02bac",
   "metadata": {},
   "outputs": [],
   "source": [
    " def say(text):\n",
    "    \n",
    "    import pyttsx3\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty('rate', 150)\n",
    "    v=engine.getProperty('voices')\n",
    "    \n",
    "    engine.setProperty('voice',v[11].id )\n",
    "    \n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "    return \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabb1a29-4785-421c-a20d-b2a885a25395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "output_file = os.path.join(\"/home/mhsn/Files/PFA/Output\", \"detected_frame.jpg\")\n",
    "i\n",
    "# Define the mapping dictionary\n",
    "class_value_mapping = {\n",
    "    0: 10,  # Class 0 corresponds to 10dt\n",
    "    1: 20,  # Class 1 corresponds to 20dt\n",
    "    2: 50,  # Class 2 corresponds to 50dt\n",
    "    3: 5    # Class 3 corresponds to 5dt\n",
    "}\n",
    "\n",
    "# Load the pre-trained model\n",
    "# Assuming model is already loaded and ready for inference\n",
    "\n",
    "# Open a video capture device (webcam or video file)\n",
    "cap = cv2.VideoCapture(0)  # Change to 0 for webcam, or provide video file path\n",
    "\n",
    "# Check if the capture device is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Failed to open video capture device.\")\n",
    "    exit()\n",
    "\n",
    "# Loop for real-time processing\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame is empty (end of video)\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform inference on the frame\n",
    "    result = model(frame)\n",
    "\n",
    "    # Assuming result.pred[0] contains the tensor you provided\n",
    "    predictions = result.pred[0]\n",
    "\n",
    "    # Flag to check if any detection occurred in the frame\n",
    "    detection_occurred = False\n",
    "\n",
    "    # Iterate through each prediction\n",
    "    for pred in predictions:\n",
    "        # Extract confidence score from the prediction\n",
    "        confidence = pred[4].item()\n",
    "\n",
    "        # Only process predictions with confidence over 0.6\n",
    "        if confidence > 0.3:\n",
    "            # Extract bounding box coordinates from the prediction\n",
    "            x1, y1, x2, y2 = map(int, pred[:4].tolist())\n",
    "\n",
    "            # Extract class index from the prediction\n",
    "            class_index = int(pred[-1].item())\n",
    "\n",
    "            # Retrieve the corresponding value from the mapping dictionary\n",
    "            value = class_value_mapping.get(class_index)\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "            # Display class label and value\n",
    "            label = f\"Class: {class_index}, Value: {value} dt, Confidance: {confidence}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Set detection occurred flag to true\n",
    "            detection_occurred = True\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # text to speech\n",
    "    if detection_occurred:\n",
    "        fp=str(value)+'Dinars'\n",
    "        say(fp)        \n",
    "        \n",
    "\n",
    "    # Check for 'q' key press to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture device and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79eb1cab-c79e-453e-9597-e4f24d12f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mhsn/Files/PFA/detect.py\", line 48, in <module>\n",
      "    from models.common import DetectMultiBackend\n",
      "ModuleNotFoundError: No module named 'models'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python3 detect.py --weights \"/home/mhsn/Files/PFA/yolov5/best.pt\" --source 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6d7b55-5bd6-4f0a-8acd-0341afe8a204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mhsn/Files/PFA\n"
     ]
    }
   ],
   "source": [
    "!cd /home/mhsn/Files/PFA/yolov5/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0873c5e-58f7-41ce-a22b-71b97090e664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ## Money Detection System\n",
      "\n",
      "    This repository contains a computer vision-based money detection system designed to assist visually impaired individuals by identifying the value of bills shown to a camera. The system uses YOLOv5 for object detection and includes a text-to-speech feature for vocal feedback.\n",
      "\n",
      "    ### Features\n",
      "    - **Object Detection**: Detects the value of bills using a trained YOLOv5 model.\n",
      "    - **Text-to-Speech**: Announces the detected bill value aloud to assist visually impaired users.\n",
      "    - **Classification Model**: Includes a ResNet-50 classification model for additional purposes.\n",
      "\n",
      "    ### Data Collection\n",
      "    The data used for training was manually captured. Each image of a bill was taken in various lighting conditions and angles to ensure robustness in real-world scenarios.\n",
      "\n",
      "    ### Data Labeling\n",
      "    Labeling was performed using [Label Studio](https://labelstud.io/), an open-source data labeling tool. The annotations are saved in `.txt` format compatible with YOLOv5.\n",
      "\n",
      "    ### Model Training\n",
      "    To train the YOLOv5 model:\n",
      "    1. Clone the YOLOv5 repository and navigate to it:\n",
      "        ```bash\n",
      "        git clone https://github.com/ultralytics/yolov5\n",
      "        cd yolov5\n",
      "        pip install -r requirements.txt\n",
      "        ```\n",
      "    2. Prepare the data folder structure and split the data into training and validation sets as shown in the notebook.\n",
      "    3. Run the training command:\n",
      "        ```bash\n",
      "        python3 train.py --img 243 --epochs 30 --data dataset.yaml --weights yolov5s.pt --cache\n",
      "        ```\n",
      "    4. **Note**: Make sure to apply necessary modifications to the YOLOv5 repository to suit the specific requirements of this project.\n",
      "\n",
      "    ### Detection\n",
      "    The detection script is implemented in `detect.py`. It uses the trained YOLOv5 model to identify bills in real-time. The script also includes a text-to-speech feature, which announces the detected value to the user.\n",
      "\n",
      "    To run detection:\n",
      "    ```bash\n",
      "    python detect.py --source 0  # Replace 0 with the path to the video file or camera index\n",
      "    ```\n",
      "\n",
      "    ### Modifications to YOLOv5 Repository\n",
      "    Several changes were made to the YOLOv5 codebase to integrate the text-to-speech feature and optimize the detection process. Please review the `detect.py` file for specific changes.\n",
      "\n",
      "    ### Additional Models\n",
      "    The repository also includes a simple classification model based on ResNet-50, available in the `model_classification` notebook.\n",
      "\n",
      "    ### Requirements\n",
      "    - Python 3.x\n",
      "    - PyTorch\n",
      "    - YOLOv5 dependencies\n",
      "    - Text-to-Speech libraries (e.g., `pyttsx3` or `gTTS`)\n",
      "\n",
      "    ### Future Work\n",
      "    - Enhance model accuracy with more diverse data.\n",
      "    - Expand support to detect additional currencies.\n",
      "\n",
      "    ### Contributing\n",
      "    Contributions are welcome! Please feel free to submit a Pull Request.\n",
      "\n",
      "    ### Contact\n",
      "    For any inquiries, please contact Rayen Souli:\n",
      "    - **Email**: soulirayen@gmail.com\n",
      "    - **LinkedIn**: [Rayen Souli](https://www.linkedin.com/rayen-souli)\n",
      "\n",
      "    ### License\n",
      "    This project is licensed under the ENSTAB License.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Python Script to Print README\n",
    "\n",
    "def print_readme():\n",
    "    readme_content = \"\"\"\n",
    "    ## Money Detection System\n",
    "\n",
    "    This repository contains a computer vision-based money detection system designed to assist visually impaired individuals by identifying the value of bills shown to a camera. The system uses YOLOv5 for object detection and includes a text-to-speech feature for vocal feedback.\n",
    "\n",
    "    ### Features\n",
    "    - **Object Detection**: Detects the value of bills using a trained YOLOv5 model.\n",
    "    - **Text-to-Speech**: Announces the detected bill value aloud to assist visually impaired users.\n",
    "    - **Classification Model**: Includes a ResNet-50 classification model for additional purposes.\n",
    "\n",
    "    ### Data Collection\n",
    "    The data used for training was manually captured. Each image of a bill was taken in various lighting conditions and angles to ensure robustness in real-world scenarios.\n",
    "\n",
    "    ### Data Labeling\n",
    "    Labeling was performed using [Label Studio](https://labelstud.io/), an open-source data labeling tool. The annotations are saved in `.txt` format compatible with YOLOv5.\n",
    "\n",
    "    ### Model Training\n",
    "    To train the YOLOv5 model:\n",
    "    1. Clone the YOLOv5 repository and navigate to it:\n",
    "        ```bash\n",
    "        git clone https://github.com/ultralytics/yolov5\n",
    "        cd yolov5\n",
    "        pip install -r requirements.txt\n",
    "        ```\n",
    "    2. Prepare the data folder structure and split the data into training and validation sets as shown in the notebook.\n",
    "    3. Run the training command:\n",
    "        ```bash\n",
    "        python3 train.py --img 243 --epochs 30 --data dataset.yaml --weights yolov5s.pt --cache\n",
    "        ```\n",
    "    4. **Note**: Make sure to apply necessary modifications to the YOLOv5 repository to suit the specific requirements of this project.\n",
    "\n",
    "    ### Detection\n",
    "    The detection script is implemented in `detect.py`. It uses the trained YOLOv5 model to identify bills in real-time. The script also includes a text-to-speech feature, which announces the detected value to the user.\n",
    "\n",
    "    To run detection:\n",
    "    ```bash\n",
    "    python detect.py --source 0  # Replace 0 with the path to the video file or camera index\n",
    "    ```\n",
    "\n",
    "    ### Modifications to YOLOv5 Repository\n",
    "    Several changes were made to the YOLOv5 codebase to integrate the text-to-speech feature and optimize the detection process. Please review the `detect.py` file for specific changes.\n",
    "\n",
    "    ### Additional Models\n",
    "    The repository also includes a simple classification model based on ResNet-50, available in the `model_classification` notebook.\n",
    "\n",
    "    ### Requirements\n",
    "    - Python 3.x\n",
    "    - PyTorch\n",
    "    - YOLOv5 dependencies\n",
    "    - Text-to-Speech libraries (e.g., `pyttsx3` or `gTTS`)\n",
    "\n",
    "    ### Future Work\n",
    "    - Enhance model accuracy with more diverse data.\n",
    "    - Expand support to detect additional currencies.\n",
    "\n",
    "    ### Contributing\n",
    "    Contributions are welcome! Please feel free to submit a Pull Request.\n",
    "\n",
    "    ### Contact\n",
    "    For any inquiries, please contact Rayen Souli:\n",
    "    - **Email**: soulirayen@gmail.com\n",
    "    - **LinkedIn**: [Rayen Souli](https://www.linkedin.com/rayen-souli)\n",
    "\n",
    "    ### License\n",
    "    This project is licensed under the ENSTAB License.\n",
    "    \"\"\"\n",
    "    print(readme_content)\n",
    "\n",
    "# Call the function to print the README content\n",
    "print_readme()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4aff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
